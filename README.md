# Time Space Explorer — A Digital Twin Interface

**Time Space Explorer** is a speculative interface prototype designed as a thought exercise — a glimpse into what interacting with a highly intelligent, fully recorded digital twin of a real-world environment might look and feel like.

Imagine something out of *Westworld*, where every moment is recorded, every interaction spatially mapped, and every sensor input logged. This tool explores how we might navigate such a data-rich model across **space**, **time**, and **sensor modalities** — not just observing reality, but interrogating it.

---

## 🌐 Live Demo

👉 [Launch Time Space Explorer](https://chethn.github.io/timespaceexplorer/index.html)

---

## 🧠 Conceptual Overview

This prototype is not a literal product — it's a **UI and interaction concept** for:
- Navigating time-stamped spatial data
- Switching between sensory "lenses" (thermal, infrared, etc.)
- Exploring layers of recorded activity and metadata
- Surfacing patterns, anomalies, and events in space-time

Think of it as a sandbox for exploring the UI/UX requirements of a future where **everything is recorded** — and insight depends on how well we can query the reality we've captured.

---

## 🔍 Interface Functions

### 🧭 Focus Control
Use the `Focus on` panel to zoom into specific layers or the exterior:
- Individual floors (`Level 1–8`)
- Entire structure (`Exterior`)
- Auto-adjusts the 3D viewport and overlays for clarity

---

### 🎥 Field of View
Toggle visibility of different capture types:
- **Photos** – Static visual documentation
- **Videos** – Dynamic recordings
- **Panos** – 360° spatial context
- **Walks** – Sequential motion capture (e.g. drone/robot paths)

Each point in the space reflects a moment in time and media type.

---

### 🕶️ Sensor Lenses
Switch between different sensor modalities. Each "lens" filters or recolors the data spatially:
- **Lens 1–5**: Represent thermal, infrared, anomaly detection, moisture mapping, structural stress, etc.
- Stackable toggles allow complex sensor blending (e.g. “Thermal + Anomaly”).

The term “lens” is used in the **AR/hud metaphor** — like soldier-grade battlefield overlays or Iron Man’s HUD.

---

### 🗓️ Time Navigation
A core component of the experience:
- **Scrub timeline** to move through the building’s life
- Use **"Age by"** filters to focus on recent or aged data
- Enable **"Time lapse"** for auto-playback across time
- Time is spatially contextual — dots appear/disappear as data is aged in or out

---

### 🧰 Additional Concepts (Planned or Imagined)
These aren’t yet implemented, but align with the thought experiment:
- 🔁 **Delta View** – See what’s changed between two time slices
- 🧠 **Cognitive Lens** – Show only anomalies detected by AI
- 🧍 **Behavioral Heatmaps** – Trace movement or usage patterns
- 🛠️ **Maintenance Overlay** – Highlight zones of degradation or scheduled service
- 📡 **Sensor Feed View** – Tap into raw sensor streams in real time

---

## 🎯 Why This Exists

This is not a commercial tool. It’s a **design sketch in code** — an attempt to explore the following questions:
- What UI constructs do we need to traverse space + time + modality?
- How do you design controls for navigating *recorded reality*?
- How do you make thousands of data points **readable**, **meaningful**, and **actionable**?

If we had access to *perfect memory of physical spaces*, this is one possible interface we’d need to make sense of it.

---

## 🧪 Try It Yourself

1. Open the [Live Demo](https://chethn.github.io/timespaceexplorer/index.html)
2. Toggle between lenses, floors, and time slices
3. Ask yourself: *What more would I need to truly understand this space?*

---

## 📄 License

MIT License © 2025 [Chethan Chandrahasan](https://github.com/chethn)

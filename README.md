# Time Space Explorer â€” A Digital Twin Interface

**Time Space Explorer** is a speculative interface prototype designed as a thought exercise â€” a glimpse into what interacting with a highly intelligent, fully recorded digital twin of a real-world environment might look and feel like.

Imagine something out of *Westworld*, where every moment is recorded, every interaction spatially mapped, and every sensor input logged. This tool explores how we might navigate such a data-rich model across **space**, **time**, and **sensor modalities** â€” not just observing reality, but interrogating it.

---

## ğŸŒ Live Demo

ğŸ‘‰ [Launch Time Space Explorer](https://chethn.github.io/timespaceexplorer/index.html)

---

## ğŸ§  Conceptual Overview

This prototype is not a literal product â€” it's a **UI and interaction concept** for:
- Navigating time-stamped spatial data
- Switching between sensory "lenses" (thermal, infrared, etc.)
- Exploring layers of recorded activity and metadata
- Surfacing patterns, anomalies, and events in space-time

Think of it as a sandbox for exploring the UI/UX requirements of a future where **everything is recorded** â€” and insight depends on how well we can query the reality we've captured.

---

## ğŸ” Interface Functions

### ğŸ§­ Focus Control
Use the `Focus on` panel to zoom into specific layers or the exterior:
- Individual floors (`Level 1â€“8`)
- Entire structure (`Exterior`)
- Auto-adjusts the 3D viewport and overlays for clarity

---

### ğŸ¥ Field of View
Toggle visibility of different capture types:
- **Photos** â€“ Static visual documentation
- **Videos** â€“ Dynamic recordings
- **Panos** â€“ 360Â° spatial context
- **Walks** â€“ Sequential motion capture (e.g. drone/robot paths)

Each point in the space reflects a moment in time and media type.

---

### ğŸ•¶ï¸ Sensor Lenses
Switch between different sensor modalities. Each "lens" filters or recolors the data spatially:
- **Lens 1â€“5**: Represent thermal, infrared, anomaly detection, moisture mapping, structural stress, etc.
- Stackable toggles allow complex sensor blending (e.g. â€œThermal + Anomalyâ€).

The term â€œlensâ€ is used in the **AR/hud metaphor** â€” like soldier-grade battlefield overlays or Iron Manâ€™s HUD.

---

### ğŸ—“ï¸ Time Navigation
A core component of the experience:
- **Scrub timeline** to move through the buildingâ€™s life
- Use **"Age by"** filters to focus on recent or aged data
- Enable **"Time lapse"** for auto-playback across time
- Time is spatially contextual â€” dots appear/disappear as data is aged in or out

---

### ğŸ§° Additional Concepts (Planned or Imagined)
These arenâ€™t yet implemented, but align with the thought experiment:
- ğŸ” **Delta View** â€“ See whatâ€™s changed between two time slices
- ğŸ§  **Cognitive Lens** â€“ Show only anomalies detected by AI
- ğŸ§ **Behavioral Heatmaps** â€“ Trace movement or usage patterns
- ğŸ› ï¸ **Maintenance Overlay** â€“ Highlight zones of degradation or scheduled service
- ğŸ“¡ **Sensor Feed View** â€“ Tap into raw sensor streams in real time

---

## ğŸ¯ Why This Exists

This is not a commercial tool. Itâ€™s a **design sketch in code** â€” an attempt to explore the following questions:
- What UI constructs do we need to traverse space + time + modality?
- How do you design controls for navigating *recorded reality*?
- How do you make thousands of data points **readable**, **meaningful**, and **actionable**?

If we had access to *perfect memory of physical spaces*, this is one possible interface weâ€™d need to make sense of it.

---

## ğŸ§ª Try It Yourself

1. Open the [Live Demo](https://chethn.github.io/timespaceexplorer/index.html)
2. Toggle between lenses, floors, and time slices
3. Ask yourself: *What more would I need to truly understand this space?*

---

## ğŸ“„ License

MIT License Â© 2025 [Chethan Chandrahasan](https://github.com/chethn)
